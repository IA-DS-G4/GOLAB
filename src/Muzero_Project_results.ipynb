{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from nn_models import Network\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from muzeroconfig import MuZeroConfig\n",
    "from nn_models import Network, NetworkOutput\n",
    "import copy\n",
    "from typing import List\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>MuZero Agent playing Boardgames</h1>\n",
    "<h2>Class Wrappers for general boardgame usage</h2>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c527baab4d6af2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we define the abstract class Wrappers to store and describe the game state for MuZero observation and action representatoin\n",
    "class Action(object):\n",
    "\n",
    "    def __init__(self, index: int):\n",
    "        # savety mechanism to ensure that the index is an integer\n",
    "        if np.issubdtype(type(index), int) :\n",
    "            self.index = index\n",
    "        else:\n",
    "            self.index = index.index\n",
    "\n",
    "    def __hash__(self):# hash function to store Actions in a dictionary\n",
    "        return self.index\n",
    "\n",
    "    def __eq__(self, other):# equality test to compare Actions with eachother\n",
    "        return self.index == other.index\n",
    "\n",
    "    def __gt__(self, other): # non-equality test to compare Actions with eachother\n",
    "        return self.index > other.index\n",
    "\n",
    "    def __str__(self): # string representation of the Action for printing\n",
    "        return str(self.index)\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "    def __init__(self, index: int):# Same as for Action class\n",
    "        self.index = index\n",
    "    def __hash__(self):\n",
    "        return self.index\n",
    "    def __eq__(self, other):\n",
    "        return self.index == other.index\n",
    "    def __gt__(self, other):\n",
    "        return self.index > other.index\n",
    "    def __str__(self):\n",
    "        return str(self.index)\n",
    "\n",
    "\n",
    "class ActionHistory(object):\n",
    "    # We store the past Actions inside this storage container to use them for training\n",
    "    def __init__(self, history: List[Action], action_space_size: int, player):\n",
    "        self.history = history\n",
    "        self.action_space_size = action_space_size\n",
    "        self.player = player\n",
    "    def clone(self): # clone the ActionHistory\n",
    "        return ActionHistory(self.history, self.action_space_size, self.player)\n",
    "    def add_action(self, action: Action): # add an Action to the history\n",
    "        self.history.append(action)\n",
    "    def last_action(self) -> Action:# return the last Action\n",
    "        return self.history[-1]\n",
    "    def action_space(self) -> List[Action]:# return the Action space\n",
    "        return [Action(i) for i in range(self.action_space_size)]\n",
    "    def to_play(self):  # return the player to play\n",
    "        return Player(self.player)\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "# Nodes represent Game states from which we can expand the search tree\n",
    "    def __init__(self, prior: float):\n",
    "# every Node has a visit count, a prior probability, a value sum, a list of children, a hidden state, a reward and a player to play\n",
    "        self.visit_count = 0\n",
    "        self.to_play = 1\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self) -> bool: # checks if the node has already been expanded\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self) -> float: # returns the value of the node\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.value_sum / self.visit_count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e467763ae0734acd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MinMaxStats:\n",
    "    \"\"\"Class For storing MinMax values of Nodes\"\"\"\n",
    "    def __init__(self):\n",
    "        self.maximum =-float('inf')\n",
    "        self.minimum = float('inf')\n",
    "    def update(self, value: float):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "    def normalize(self, value: float) -> float:\n",
    "        if self.maximum > self.minimum:\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"A class that runs Monte Carlo Tree Search. We use the UCB1 formula. to select a child and search from there\"\"\"\n",
    "    @staticmethod\n",
    "    def play_game(config: MuZeroConfig, network: Network):\n",
    "        game = config.new_game()\n",
    "\n",
    "        while not game.done and len(game.board.game_history) < config.max_moves:\n",
    "            # At the root of the search tree we use the representation function to to get a hidden state from observation\n",
    "            root = Node(0)\n",
    "            current_observation = game.get_observation()\n",
    "            # expand different board actions\n",
    "            MCTS.expand_node(root,\n",
    "                        game.to_play(),\n",
    "                        game.legal_actions(),\n",
    "                        network.initial_inference(current_observation))\n",
    "            MCTS.add_exploration_noise(config, root)\n",
    "            # running the MCTS using the network for evaluating positions and choosing actions\n",
    "            MCTS.run_mcts(config,root, game, network)\n",
    "            action = MCTS.select_action(config, root, network)\n",
    "            game.store_search_statistics(root)\n",
    "            game.apply(action)\n",
    "        return game\n",
    "\n",
    "    @staticmethod\n",
    "    def run_mcts(config: MuZeroConfig,\n",
    "                 root: Node,\n",
    "                 game,\n",
    "                 network: Network):\n",
    "        # store minmax values\n",
    "        min_max_stats = MinMaxStats()\n",
    "        #copy game state to avoid changing the original game when running MCTS\n",
    "        game_copy = copy.deepcopy(game)\n",
    "        # we simulate the game for congig.num_simulations times\n",
    "        for _ in range(config.num_simulations):\n",
    "\n",
    "            history = game_copy.get_action_history()\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            while node.expanded():\n",
    "                action, node = MCTS.select_child(config, node, min_max_stats)\n",
    "                game_copy.apply(action)\n",
    "                history.add_action(action)\n",
    "                search_path.append(node)\n",
    "            # When traversing the tree we use the dynamics network to predict next states\n",
    "            parent = search_path[-2]\n",
    "            network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                         history.last_action())\n",
    "            (MCTS.expand_node(node, history.to_play(), game_copy.legal_actions(), network_output))\n",
    "            MCTS.backpropagate(search_path,\n",
    "                          network_output.value,\n",
    "                          history.to_play(),\n",
    "                          config.discount,\n",
    "                          min_max_stats)\n",
    "            if game_copy.done:\n",
    "                break\n",
    "        del game_copy\n",
    "    @staticmethod # take a softmax sample from a given distribution\n",
    "    def softmax_sample(distribution, temperature: float):\n",
    "        visit_counts = np.array([visit_counts for _ , visit_counts in distribution])\n",
    "        actions = np.array([actions for actions , _ in distribution])\n",
    "        visit_counts_exp = np.exp(visit_counts)\n",
    "        policy = visit_counts_exp / np.sum(visit_counts_exp)\n",
    "        policy = (policy ** (1 / temperature)) / (policy ** (1 / temperature)).sum()\n",
    "        action_index = np.random.choice(actions, p=policy)\n",
    "\n",
    "        return action_index\n",
    "    @staticmethod\n",
    "    def select_action(config,\n",
    "                      node: Node,\n",
    "                      network: Network) -> Action:\n",
    "        visit_counts = [(action,child.visit_count) for action, child in node.children.items()]\n",
    "        t = config.visit_softmax_temperature(training_steps=network.training_steps())\n",
    "        action = Action(MCTS.softmax_sample(visit_counts, t))\n",
    "        return action\n",
    "\n",
    "    # Select the child with the highest UCB score.\n",
    "    @staticmethod\n",
    "    def select_child(config: MuZeroConfig, node: Node, min_max_stats: MinMaxStats):\n",
    "        _, action, child = max(\n",
    "            (MCTS.ucb_score(config, node, child, min_max_stats), action, child) for action, child in node.children.items())\n",
    "        return action, child\n",
    "\n",
    "    # The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
    "    @staticmethod\n",
    "    def ucb_score(config: MuZeroConfig, parent: Node, child: Node, min_max_stats: MinMaxStats) -> float:\n",
    "        pb_c = math.log((parent.visit_count + config.pb_c_base + 1) / config.pb_c_base) + config.pb_c_init\n",
    "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "        prior_score = pb_c * child.prior\n",
    "        if child.visit_count > 0:\n",
    "            value_score = min_max_stats.normalize(child.reward + config.discount * child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "        return prior_score + value_score\n",
    "\n",
    "    # We expand a node using the value, reward and policy prediction obtained from the neural network.\n",
    "    @staticmethod\n",
    "    def expand_node(node: Node, to_play: Player, actions: List[Action], network_output: NetworkOutput):\n",
    "        node.to_play = to_play\n",
    "        node.hidden_state = network_output.hidden_state\n",
    "        node.reward = network_output.reward\n",
    "        policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "        policy_sum = sum(policy.values())\n",
    "        for action, p in policy.items():\n",
    "            node.children[action] = Node(p / policy_sum)\n",
    "\n",
    "    # At the end of a simulation, we propagate the evaluation all the way up the tree to the root.\n",
    "    @staticmethod\n",
    "    def backpropagate(search_path: List[Node], value: float, to_play: Player, discount: float,\n",
    "                      min_max_stats: MinMaxStats):\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value if node.to_play == to_play else -value\n",
    "            node.visit_count += 1\n",
    "            min_max_stats.update(node.value())\n",
    "\n",
    "            value = node.reward + discount * value\n",
    "\n",
    "\n",
    "    @staticmethod # We add noise to the distribution to encourage exploration\n",
    "    def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "        actions = list(node.children.keys())\n",
    "        noise = np.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "        frac = config.root_exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "class NetworkStorage(object):\n",
    "    def __init__(self, config):\n",
    "        self.network = Network(config)\n",
    "\n",
    "    def save_network(self,network):\n",
    "        self.network = network\n",
    "\n",
    "    def latest_network(self):\n",
    "        return self.network\n",
    "\n",
    "\n",
    "\n",
    "class GameStorage(object):\n",
    "    def __init__(self, config: MuZeroConfig):\n",
    "        self.window_size = config.window_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.store = []\n",
    "\n",
    "    def save_game(self, game):\n",
    "        if len(self.store) > self.window_size:\n",
    "            self.store.pop(0)\n",
    "        self.store.append(game)\n",
    "\n",
    "    def sample_batch(self, num_unroll_steps: int, td_steps: int, action_space_size: int):\n",
    "        games = [self.sample_game() for _ in range(self.batch_size)]\n",
    "        game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "        return [(g.observation_list[i],\n",
    "                 g.action_history[i:i + num_unroll_steps],\n",
    "                 g.make_target(state_index=i,num_unroll_steps=num_unroll_steps, td_steps=td_steps))\n",
    "                for (g, i) in game_pos]\n",
    "\n",
    "    def sample_game(self):\n",
    "        # get games from the gamestorage\n",
    "        return self.store[np.random.choice(range(len(self.store)))]\n",
    "\n",
    "    def sample_position(self, game) -> int:\n",
    "        # sample a postion from the game\n",
    "        return np.random.choice(range(len(game.rewards) - 1))\n",
    "\n",
    "    def last_game(self):\n",
    "        return self.store[-1]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87ebe124c0633d03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Game specific MuZero configuration</h2>\n",
    "this is the parent class for creating a Game-Dependent Muzero Agent configuration. we inherit from this class to build agents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c0c26e345f6315"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MuZeroConfig(object):\n",
    "    def __init__(self,\n",
    "                 action_space_size: int,\n",
    "                 observation_space_size: int,\n",
    "                 observation_space_shape: (int,int),\n",
    "                 max_moves: int,\n",
    "                 discount: float,\n",
    "                 dirichlet_alpha: float,\n",
    "                 num_simulations: int,\n",
    "                 batch_size: int,\n",
    "                 td_steps: int,\n",
    "                 num_actors: int,\n",
    "                 lr_init: float,\n",
    "                 lr_decay_steps: float,\n",
    "                 dropout_rate: float,\n",
    "                 training_episodes: int,\n",
    "                 hidden_layer_size: int,\n",
    "                 model_name: str,\n",
    "                 visit_softmax_temperature,\n",
    "                 ):\n",
    "        ### Self-Play\n",
    "        self.action_space_size = action_space_size\n",
    "        self.observation_space_size = observation_space_size\n",
    "        self.observation_space_shape = observation_space_shape\n",
    "        self.num_actors = num_actors\n",
    "\n",
    "        self.visit_softmax_temperature = visit_softmax_temperature\n",
    "        self.max_moves = max_moves\n",
    "        self.num_simulations = num_simulations\n",
    "        self.discount = discount\n",
    "\n",
    "        # Root prior exploration noise.\n",
    "        self.root_dirichlet_alpha = dirichlet_alpha\n",
    "        self.root_exploration_fraction = 0.25\n",
    "\n",
    "        # UCB formula\n",
    "        self.pb_c_base = 19652\n",
    "        self.pb_c_init = 1.25\n",
    "\n",
    "        ### Training\n",
    "        # We discard old games after every bage to save on RAM memory\n",
    "        self.window_size = batch_size\n",
    "        self.batch_size = batch_size\n",
    "        # num of steps top unroll for MCTS\n",
    "        self.num_unroll_steps = 3\n",
    "        self.td_steps = td_steps\n",
    "        # optimization parameters\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "        self.training_episodes = training_episodes\n",
    "        # Network params\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Exponential learning rate schedule\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "        self.lr_decay_rate = 0.1\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def new_game(self):\n",
    "        return Game(self.action_space_size, self.discount)\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, action_space_size, discount):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.discount = discount\n",
    "        self.observation_history = []\n",
    "        self.action_history = []\n",
    "        self.rewards = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb854c8da56953d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from go_board import GoBoard\n",
    "from go_utils import GoUtils\n",
    "# import Go implementation\n",
    "\n",
    "class Go7x7Config(MuZeroConfig):\n",
    "# inherit from configuration class\n",
    "    def new_game(self): # function to create a new game\n",
    "        return Go7x7()\n",
    "\n",
    "# pass the configuration for the game sepcific agent\n",
    "def make_Go7x7_config() -> MuZeroConfig:\n",
    "    # we use a temperature schedule to control the exploration rate of the agent\n",
    "    def visit_softmax_temperature(training_steps):\n",
    "\n",
    "        if training_steps < 100:\n",
    "            return 3\n",
    "        elif training_steps < 125:\n",
    "            return 2\n",
    "        elif training_steps < 150:\n",
    "            return 1\n",
    "        elif training_steps < 175:\n",
    "            return 0.5\n",
    "        elif training_steps < 200:\n",
    "            return 0.250\n",
    "        elif training_steps < 225:\n",
    "            return 0.125\n",
    "        else:\n",
    "            return 0.125\n",
    "\n",
    "    return Go7x7Config(action_space_size= 50,\n",
    "                        observation_space_size= 49,\n",
    "                        observation_space_shape= (7,7),\n",
    "                        max_moves=98,\n",
    "                        discount=0.999,\n",
    "                        dropout_rate = 0.1,\n",
    "                        dirichlet_alpha=0.25,\n",
    "                        num_simulations=10,\n",
    "                        batch_size=16,\n",
    "                        td_steps=25,\n",
    "                        lr_init=0.001,\n",
    "                        lr_decay_steps=10,\n",
    "                        training_episodes=500,\n",
    "                        hidden_layer_size= 49,\n",
    "                        visit_softmax_temperature=visit_softmax_temperature,\n",
    "                        num_actors=2,\n",
    "                        model_name=\"Go7x7\")\n",
    "\n",
    "# This function is used to create a new game with which the agent can interact\n",
    "class Go7x7:\n",
    "    def __init__(self):\n",
    "        self.board_size = 7\n",
    "        self.player = 1 # Black goes first\n",
    "        self.board = GoBoard(board_dimension=self.board_size, player=self.player) # from go_board.py\n",
    "        self.utils = GoUtils() # from go_utils.py\n",
    "        self.observation_space_shape = (self.board_size,self.board_size)\n",
    "        self.observation_space_size = self.board_size**2\n",
    "        self.action_space_size = (self.board_size**2)+1\n",
    "        self.action_history = []\n",
    "        # sto store rewards for training\n",
    "        self.rewards = []\n",
    "        # to store board states for training\n",
    "        self.observation_list = []\n",
    "        # to store the visit counts and values of the nodes\n",
    "        self.child_visits = []\n",
    "        self.root_values = []\n",
    "        self.discount = 0.999\n",
    "        self.done = False\n",
    "   \n",
    "   # does a step in the game\n",
    "    def step(self, action):\n",
    "        r = int(np.floor(action / self.board_size))\n",
    "        c = int(action % self.board_size)\n",
    "        move = (r,c)\n",
    "        if action == self.board_size**2:\n",
    "            move = (-1,-1)\n",
    "            #check if move is viable\n",
    "        move_viable, self.board = self.utils.make_move(board=self.board,move=move)\n",
    "        # punish the agent if he does an illegal move\n",
    "        if not move_viable:\n",
    "            done = True\n",
    "            reward = -1\n",
    "            return self.get_observation(), reward, done\n",
    "        # check if the game is finished by definition (2 passes)\n",
    "        done = self.utils.is_game_finished(board=self.board)\n",
    "        if done and move_viable:\n",
    "            reward = 1 if self.utils.evaluate_winner(board_grid=self.board.board_grid)[0] == self.player else -1\n",
    "        elif not done and move_viable:\n",
    "            reward = 0\n",
    "        return self.get_observation(), reward, done\n",
    "    # apply one interaction between agent and environment, store the reward and observation\n",
    "    def apply(self, action: Action):\n",
    "        observation, reward, done = self.step(action.index)\n",
    "        self.rewards.append(reward)\n",
    "        self.action_history.append(action.index)\n",
    "        self.observation_list.append(observation)\n",
    "        self.done = done\n",
    "    \n",
    "    # get the observation of the current game state\n",
    "    def get_observation(self):\n",
    "        return tf.constant([self.board.board_grid],dtype=\"int32\")\n",
    "    # return the legal actions for the current game state\n",
    "    def legal_actions(self)-> List[Action]:\n",
    "        # Pass = boardsize**2 is always legal\n",
    "        legal = [self.board_size**2]\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if self.utils.is_valid_move(board=self.board,move=(i,j)):\n",
    "                    legal.append(i * self.board_size + j)\n",
    "        return [Action(index) for index in legal]\n",
    "    # return the the total reward of the game\n",
    "    def total_rewards(self):\n",
    "        return sum(self.rewards)\n",
    "    # return if game is finished\n",
    "    def is_finished(self):\n",
    "        finished = self.utils.is_game_finished(board=self.board)\n",
    "        if finished:\n",
    "            print(\"game is finished!\")\n",
    "        return finished\n",
    "    # return the player to play\n",
    "    def to_play(self):\n",
    "        return Player(self.board.player)\n",
    "    # return the action history\n",
    "    def get_action_history(self) -> ActionHistory:\n",
    "\n",
    "        return ActionHistory(self.action_history, self.action_space_size,self.board.player)\n",
    "    # store the visit counts and values of the nodes\n",
    "    def store_search_statistics(self, root: Node):\n",
    "        # We store the visited board positions and their values for later training\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        action_space = (Action(action) for action in range(self.action_space_size))\n",
    "        self.child_visits.append([\n",
    "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "            for a in action_space\n",
    "        ])\n",
    "        self.root_values.append(root.value())\n",
    "    # return the targets for training\n",
    "    def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int):\n",
    "        # Bootsstapping technique to calculate the value of a state\n",
    "        targets = []\n",
    "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
    "            bootstrap_index = current_index + td_steps\n",
    "            if bootstrap_index < len(self.root_values):\n",
    "                value = self.root_values[bootstrap_index] * self.discount ** td_steps\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "            for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
    "                value += reward * self.discount ** i  # pytype: disable=unsupported-operands\n",
    "\n",
    "            if current_index > 0 and current_index <= len(self.rewards):\n",
    "                last_reward = self.rewards[current_index - 1]\n",
    "            else:\n",
    "                last_reward = 0\n",
    "\n",
    "            if current_index < len(self.root_values):\n",
    "                targets.append((value, last_reward, self.child_visits[current_index]))\n",
    "            else:\n",
    "                # States past the end of games are treated as absorbing states.\n",
    "                targets.append((0, last_reward, []))\n",
    "        return targets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9ecbef3a95417e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We created the same, uncommented configuration file for the Go9x9 game\n",
    "<h2>Network Architecture</h2>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f193738367b5ed3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Wrappers import Action\n",
    "from typing import NamedTuple, Dict, List\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.python.keras.regularizers import L2\n",
    "\n",
    "\n",
    "'''\n",
    "Impementation of neural network in muzero algorithm for 7x7 game board\n",
    "\n",
    "There are 4 networks:\n",
    "- The Representation network \n",
    "- The Value network\n",
    "- The Policy network\n",
    "- The Reward network \n",
    "'''\n",
    "# Avstract class for the network output\n",
    "class NetworkOutput(NamedTuple):\n",
    "    value: float\n",
    "    reward: float\n",
    "    policy_logits: Dict[Action, float]\n",
    "    policy_tensor: List[float]\n",
    "    hidden_state: List[float]\n",
    "\n",
    "# Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.hidden_layer_size = config.hidden_layer_size\n",
    "        self.observation_space_shape = (1,) + config.observation_space_shape + (1,)\n",
    "        regularizer = L2(config.weight_decay)\n",
    "\n",
    "\n",
    "        #state encoder conv network\n",
    "        self.representation = keras.Sequential()\n",
    "        self.representation.add(layers.Conv2D(64, (3, 3),activation='relu',kernel_regularizer=regularizer))\n",
    "        self.representation.add(layers.Conv2D(64, (2, 2), activation='relu', kernel_regularizer=regularizer))\n",
    "        self.representation.add(layers.Conv2D(64, (2, 2), activation='relu', kernel_regularizer=regularizer))\n",
    "        self.representation.add(layers.Flatten())\n",
    "        self.representation.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.representation.add(layers.Dropout(config.dropout_rate))\n",
    "        self.representation.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.representation.add(layers.Dropout(config.dropout_rate))\n",
    "        self.representation.add(layers.Dense(config.hidden_layer_size, kernel_regularizer=regularizer))\n",
    "\n",
    "        #value network MLP\n",
    "        self.value = keras.Sequential()\n",
    "        self.value.add(layers.Dense(config.hidden_layer_size, activation='relu', kernel_regularizer=regularizer))\n",
    "        self.value.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.value.add(layers.Dropout(config.dropout_rate))\n",
    "        self.value.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.value.add(layers.Dropout(config.dropout_rate))\n",
    "        self.value.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.value.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.value.add(layers.Dense(1,activation='relu', kernel_regularizer=regularizer))\n",
    "\n",
    "        # policy network conv\n",
    "        self.policy = keras.Sequential()\n",
    "        self.policy.add(layers.Dense(config.hidden_layer_size, activation='relu', kernel_regularizer=regularizer))\n",
    "        self.policy.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.policy.add(layers.Dropout(config.dropout_rate))\n",
    "        self.policy.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.policy.add(layers.Dropout(config.dropout_rate))\n",
    "        self.policy.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.policy.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.policy.add(layers.Dense(config.action_space_size, activation='softmax', kernel_regularizer=regularizer))\n",
    "\n",
    "        #reward net MLP\n",
    "        self.reward = keras.Sequential()\n",
    "        self.reward.add(layers.Dense(config.hidden_layer_size, activation='relu', kernel_regularizer=regularizer))\n",
    "        self.reward.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.reward.add(layers.Dropout(config.dropout_rate))\n",
    "        self.reward.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.reward.add(layers.Dropout(config.dropout_rate))\n",
    "        self.reward.add(layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.reward.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.reward.add(layers.Dense(1,activation='relu', kernel_regularizer=regularizer))\n",
    "\n",
    "\n",
    "        self.dynamics = keras.Sequential()\n",
    "        self.dynamics.add(layers.Dense(config.hidden_layer_size, activation='relu', kernel_regularizer=regularizer))\n",
    "        self.dynamics.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.dynamics.add(layers.Dropout(0.2))\n",
    "        self.dynamics.add(layers.Dense(1024, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.dynamics.add(layers.Dropout(0.2))\n",
    "        self.dynamics.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "        self.dynamics.add(layers.Dense(config.hidden_layer_size, kernel_regularizer=regularizer))\n",
    "\n",
    "        self.compile(optimizer=keras.optimizers.legacy.Adam(),metrics=['accuracy'], loss='mean_squared_error')\n",
    "\n",
    "        self.tot_training_steps = 0\n",
    "        self.backup_count = 0\n",
    "\n",
    "        self.action_space_size = config.action_space_size\n",
    "\n",
    "    def compile(self,loss, optimizer, metrics):\n",
    "        self.dynamics.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "        self.representation.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "        self.value.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "        self.policy.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "        self.reward.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "    # for summarizing the network. Run this cell to get a summary of the network\n",
    "    def summarise(self):\n",
    "        self.representation.build(self.observation_space_shape)\n",
    "        self.value.build((0,100))\n",
    "        self.policy.build(self.observation_space_shape)\n",
    "        self.reward.build((0,100))\n",
    "        self.dynamics.build((0,100))\n",
    "\n",
    "        print(self.representation.summary(),\n",
    "        self.value.summary(),\n",
    "        self.policy.summary(),\n",
    "        self.reward.summary(),\n",
    "        self.dynamics.summary(),\n",
    "              )\n",
    "\n",
    "    # initial inference for the representation network\n",
    "    def initial_inference(self, image) -> NetworkOutput:\n",
    "        # representation + prediction function\n",
    "        image = tf.expand_dims(image, axis= 3)\n",
    "        image = tf.cast(image, dtype=tf.float32)\n",
    "        hidden_state = self.representation(image)\n",
    "\n",
    "        #check if tensor is non-zero, if so only then normalize to avoid nan's\n",
    "        zero_check = tf.reduce_all(tf.equal(hidden_state, 0.0))\n",
    "        if not zero_check:\n",
    "            hidden_state = tf.linalg.normalize(hidden_state)[0]\n",
    "\n",
    "        value = self.value(hidden_state)\n",
    "        policy = self.policy(hidden_state)\n",
    "        reward = tf.constant([[0]], dtype=tf.float32)\n",
    "        policy_p = policy[0]\n",
    "        return NetworkOutput(value,\n",
    "                             reward,\n",
    "                             {Action(a): policy_p[a] for a in range(len(policy_p))},\n",
    "                             policy,\n",
    "                             hidden_state)\n",
    "    \n",
    "# recurrent inference for the dynamics, reward, value and policy network\n",
    "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "        # dynamics + prediction function\n",
    "        a = hidden_state.numpy()[0]\n",
    "        b = np.eye(self.action_space_size)[action.index]\n",
    "        nn_input = np.concatenate((a, b))\n",
    "        nn_input = np.expand_dims(nn_input, axis=0)\n",
    "\n",
    "        next_hidden_state = self.dynamics(nn_input)\n",
    "\n",
    "        # next_hidden_state = tf.keras.utils.normalize(next_hidden_state)\n",
    "\n",
    "        reward = self.reward(nn_input)\n",
    "        value = self.value(next_hidden_state)\n",
    "        policy = self.policy(next_hidden_state)\n",
    "        policy_p = policy[0]\n",
    "\n",
    "        return NetworkOutput(value,\n",
    "                             reward,\n",
    "                             {Action(a): policy_p[a] for a in range(len(policy_p))},\n",
    "                             policy,\n",
    "                             next_hidden_state)\n",
    "\n",
    "    def get_weights(self):\n",
    "        # Returns the weights of this network.\n",
    "\n",
    "        networks = (self.representation,\n",
    "                    self.value,\n",
    "                    self.policy,\n",
    "                    self.dynamics,\n",
    "                    self.reward)\n",
    "        return [variables\n",
    "                for variables_list in map(lambda n: n.weights, networks)\n",
    "                for variables in variables_list]\n",
    "\n",
    "    def training_steps(self) -> int:\n",
    "        # How many steps / batches the network has been trained for.\n",
    "        return self.tot_training_steps\n",
    "    # save the network\n",
    "    def save_network_deepcopy(self,model_name):\n",
    "\n",
    "        self.representation.save(f\"../Saved models/{model_name}/backup{self.backup_count}/representation_network\")\n",
    "        self.value.save(f\"../Saved models/{model_name}/backup{self.backup_count}/value_network\")\n",
    "        self.dynamics.save(f\"../Saved models/{model_name}/backup{self.backup_count}/dynamics_network\")\n",
    "        self.policy.save(f\"../Saved models/{model_name}/backup{self.backup_count}/policy_network\")\n",
    "        self.reward.save(f\"../Saved models/{model_name}/backup{self.backup_count}/reward_network\")\n",
    "    # load the network\n",
    "    def load_network_deepcopy(self, model_name, backup_count):\n",
    "        self.representation = keras.models.load_model(\n",
    "            f\"../Saved models/{model_name}/backup{backup_count}/representation_network\")\n",
    "        self.value = keras.models.load_model(f\"../Saved models/{model_name}/backup{backup_count}/value_network\")\n",
    "        self.dynamics = keras.models.load_model(f\"../Saved models/{model_name}/backup{backup_count}/dynamics_network\")\n",
    "        self.policy = keras.models.load_model(f\"../Saved models/{model_name}/backup{backup_count}/policy_network\")\n",
    "        self.reward = keras.models.load_model(f\"../Saved models/{model_name}/backup{backup_count}/reward_network\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    network =Network(make_Go7x7_config())\n",
    "    #network.load_network_deepcopy(model_name=\"Go7x7\", backup_count=4) # This is how to load a network\n",
    "    network.summarise()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9437acbc364976f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Main Training Loop of the Agent</h2>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22bf19cdd6aa341c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to check and install GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"TensorFlow is using GPU: \", tf.test.is_gpu_available())\n",
    "\n",
    "# We reapeteatly create moves from mcts\n",
    "def run_selfplay(config: MuZeroConfig,\n",
    "                 storage: NetworkStorage,\n",
    "                 game_storage: GameStorage,\n",
    "                 iteration: int):\n",
    "    mcts = MCTS()\n",
    "    for _ in tqdm(range(config.batch_size), desc=\"Selfplay Batch creation\", position=1, leave=True):\n",
    "        # load network from network save\n",
    "        network = storage.latest_network()\n",
    "        game = mcts.play_game(config,network)\n",
    "        #store the game inside of a container\n",
    "        game_storage.save_game(game)\n",
    "    print(f\"Batch {iteration} creation completed. Starting training...\")\n",
    "\n",
    "\n",
    "def scalar_loss(prediction, target) -> float:\n",
    "    # calculating mean squared error\n",
    "    return tf.losses.mean_squared_error(target, prediction)\n",
    "\n",
    "def scale_gradient(tensor, scale: float):\n",
    "    # Scales the gradient for the backward pass.\n",
    "    return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)\n",
    "\n",
    "# Function to apply gradients to update weights\n",
    "def update_weights(optimizer: tf.keras.optimizers.Optimizer, network: Network, batch, weight_decay: float):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 0\n",
    "        for (image, actions, targets) in batch:\n",
    "\n",
    "            # Initial step, from the real observation.\n",
    "            value, reward, _, policy_t, hidden_state = network.initial_inference(image)\n",
    "            predictions = [(1.0, value, reward, policy_t)]\n",
    "\n",
    "            # Recurrent steps, from action and previous hidden state.\n",
    "            for action in actions:\n",
    "                value, reward, _, policy_t, hidden_state = network.recurrent_inference(hidden_state, Action(action))\n",
    "                predictions.append((1.0 / len(actions), value, reward, policy_t))\n",
    "\n",
    "                hidden_state = scale_gradient(hidden_state, 0.5)\n",
    "            # compare prediciton and target\n",
    "            for k, (prediction, target) in enumerate(zip(predictions, targets)):\n",
    "\n",
    "                gradient_scale, value, reward, policy_t = prediction\n",
    "                target_value, target_reward, target_policy = target\n",
    "\n",
    "                l_a = scalar_loss(value, [target_value])\n",
    "\n",
    "                if k > 0:\n",
    "                    l_b = tf.dtypes.cast(scalar_loss(reward, [target_reward]), tf.float32)\n",
    "                else:\n",
    "                    l_b = 0\n",
    "\n",
    "                if target_policy == []:\n",
    "                    l_c = 0\n",
    "                else:\n",
    "                    cce = keras.losses.CategoricalCrossentropy()\n",
    "                    l_c = cce([target_policy], policy_t)\n",
    "\n",
    "                l = l_a + l_b + l_c\n",
    "\n",
    "                loss += scale_gradient(l, gradient_scale)\n",
    "\n",
    "        loss /= len(batch)\n",
    "\n",
    "        for weights in network.get_weights():\n",
    "            loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # calculate gradients\n",
    "    gradients = tape.gradient(loss, [network.representation.trainable_variables,\n",
    "                                     network.dynamics.trainable_variables,\n",
    "                                     network.policy.trainable_variables,\n",
    "                                     network.value.trainable_variables,\n",
    "                                     network.reward.trainable_variables])\n",
    "    # apply grads on different networks\n",
    "    optimizer.apply_gradients(zip(gradients[0], network.representation.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[1], network.dynamics.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[2], network.policy.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[3], network.value.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[4], network.reward.trainable_variables))\n",
    "\n",
    "    return network, loss\n",
    "\n",
    "def train_network(config: MuZeroConfig, storage: NetworkStorage, game_storage:GameStorage, iterations: int):\n",
    "    network = storage.latest_network()\n",
    "    # we decrease learning rate over time to stabilize training\n",
    "    learning_rate = config.lr_init * config.lr_decay_rate ** (iterations / config.lr_decay_steps)\n",
    "    optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    # load batch from game storage for training\n",
    "    batch = game_storage.sample_batch(config.num_unroll_steps, config.td_steps, config.action_space_size)\n",
    "    network, loss = update_weights(optimizer, network, batch, config.weight_decay)\n",
    "\n",
    "    network.tot_training_steps += 1\n",
    "\n",
    "    #save model every 10 episodes\n",
    "    if iterations % 10 == 0 :\n",
    "        network.backup_count += 1\n",
    "        network.save_network_deepcopy(model_name=config.model_name)\n",
    "    # save network after training\n",
    "    storage.save_network(network)\n",
    "    # to save some RAM\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def muzero_train(config: MuZeroConfig):\n",
    "\n",
    "    model_name = config.model_name\n",
    "    storage = NetworkStorage(config)\n",
    "    game_storage = GameStorage(config)\n",
    "    losses = []\n",
    "    print(f\"Starting Selfplay for {config.model_name}! Batch size is {config.batch_size}.\")\n",
    "\n",
    "    for i in tqdm(range(config.training_episodes), desc=f\"Training episodes for {config.model_name}\", position=0):\n",
    "\n",
    "        # self-play\n",
    "        run_selfplay(config, storage, game_storage, i)\n",
    "\n",
    "        # training\n",
    "        loss = train_network(config, storage, game_storage, i)\n",
    "\n",
    "        # print and plot loss\n",
    "        print('Loss: ' + str(loss))\n",
    "        losses.append(loss[0])\n",
    "        plt.plot(losses, label=f\"Loss {model_name}\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"batches processed\")\n",
    "        plt.show()\n",
    "        plt.savefig(\"loss_plot_\" + model_name + \".png\")\n",
    "        df = pd.DataFrame(losses, columns=['loss'])\n",
    "        # saving the dataframe\n",
    "        df.to_csv(f'loss_{model_name}.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # uncomment depending on which model you want to train\n",
    "    muzero_train(make_Go7x7_config())\n",
    "    #muzero(make_Go9x9_config())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b40b9dae0cf17a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additional files which are not in the notebook but still important:\n",
    "\n",
    "go_board.py define the GoBoard class which is used to store the game state and the game history\n",
    "go_utils.py defines the GoUtils class which is used act on the board and check if moves are legal, game is over ...\n",
    "go_graphics.py defines the GoGraphics class which is used to visualize the game when played interactively\n",
    "play_human_human.py is used to play a game of Go between two human players\n",
    "play_MuZero_interactive.py is used to play a game of Go against the MuZero agent, using the interactive interface\n",
    "MuzeroAgent.py is used to play a game of Go against the MuZero agent, using the server client interaction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c29955590137e70e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Agent to play against\n",
    "class MuZeroAgent:\n",
    "    def __init__(self, config: MuZeroConfig, backup_count_to_load=1):\n",
    "        self.model_path = config.model_name\n",
    "        self.config = config\n",
    "        self.game = self.config.new_game()\n",
    "\n",
    "        # Assuming you have an instance of Network\n",
    "        self.network = Network(config=self.config)\n",
    "\n",
    "        # Load the weights for a specific model and backup\n",
    "        self.backup_count_to_load = backup_count_to_load\n",
    "\n",
    "        self.network.load_network_deepcopy(model_name=self.model_path, backup_count=self.backup_count_to_load)\n",
    "\n",
    "        # Now, the `network` instance has its weights loaded from the specified backup.\n",
    "\n",
    "    def connect_to_server(self,host='localhost', port=12345):\n",
    "        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        client_socket.connect((host, port))\n",
    "\n",
    "        response = client_socket.recv(1024).decode()\n",
    "        print(f\"Server ResponseINIT: {response}\")\n",
    "\n",
    "        Game = response[-4:]\n",
    "        print(\"Playing:\", Game)\n",
    "\n",
    "        if \"1\" in response:\n",
    "            ag = 1\n",
    "        else:\n",
    "            ag = 2\n",
    "        first = True\n",
    "\n",
    "        while True:\n",
    "            # Generate and send a random move\n",
    "\n",
    "            if ag == 1 or not first:\n",
    "                move, move_str = self.generate_move()\n",
    "                time.sleep(1)\n",
    "                client_socket.send(move_str.encode())\n",
    "                print(\"Send:\", move_str)\n",
    "\n",
    "                # Wait for server response\n",
    "                response = client_socket.recv(1024).decode()\n",
    "                move = self.decode_move_string(response)\n",
    "                print(f\"Server Response1: {response}\")\n",
    "                self.act_other_agent_move(move)\n",
    "                if \"END\" in response or self.game.is_finished(): break\n",
    "            if ag == 2:\n",
    "                response = client_socket.recv(1024).decode()\n",
    "                move = self.decode_move_string(response)\n",
    "                print(f\"Server Response1: {response}\")\n",
    "                if \"END\" in response or self.game.is_finished(): break\n",
    "                self.act_other_agent_move(move)\n",
    "                move, move_str = self.generate_move()\n",
    "                time.sleep(1)\n",
    "                client_socket.send(move_str.encode())\n",
    "                print(\"Send:\", move_str)\n",
    "\n",
    "        client_socket.close()\n",
    "\n",
    "    def act_other_agent_move(self,move):\n",
    "        action = Action(move[0]*self.game.board_size + move[1])\n",
    "        self.game.apply(action)\n",
    "\n",
    "    def generate_random_move(self):\n",
    "        x = random.randint(0, 9)\n",
    "        y = random.randint(0, 9)\n",
    "        return f\"MOVE {x},{y}\"\n",
    "\n",
    "    def generate_move(self):\n",
    "        root = Node(0)\n",
    "        current_observation = self.game.get_observation()\n",
    "        MCTS.expand_node(root,\n",
    "                         self.game.to_play(),\n",
    "                         self.game.legal_actions(),\n",
    "                         self.network.initial_inference(current_observation))\n",
    "        MCTS.run_mcts(self.config, root, self.game, self.network)\n",
    "        action = MCTS.select_action(self.config, len(self.game.action_history), root, self.network)\n",
    "        self.game.store_search_statistics(root)\n",
    "        # print(f\"move{action} from Player {game.board.player}\")\n",
    "        self.game.apply(action)\n",
    "        r = int(np.floor(action.index / self.game.board_size))\n",
    "        c = int(action.index % self.game.board_size)\n",
    "        move = (r, c)\n",
    "        if action.index == self.game.board_size ** 2:\n",
    "            move = (-1, -1)\n",
    "        return move, f\"MOVE {move[0]},{move[1]}\"\n",
    "    @staticmethod\n",
    "    def decode_move_string(move_string):\n",
    "        # Assuming move_string is in the format \"MOVE x,y\"\n",
    "        _, coordinates = move_string.split(\" \")\n",
    "        x, y = map(int, coordinates.split(\",\"))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = MuZeroAgent(config=make_Go7x7_config())\n",
    "    agent.connect_to_server()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "563d00fe50671d47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Results</h2>\n",
    "We tracked the training progress by looking at the loss of the network. We trained the network for 90 episodes for the 7x7 and 40 for the 9x9, which took about 7 hours each on my Chromebook with i5 core. The loss of the network is shown below:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a591a6de4ed67acb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e307473276c57cb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
