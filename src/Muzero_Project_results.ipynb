{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from nn_models import Network\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define classes\n",
    "MAXIMUM_FLOAT_VALUE = float('inf')\n",
    "\n",
    "class MinMaxStats:\n",
    "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maximum =-MAXIMUM_FLOAT_VALUE\n",
    "        self.minimum = MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values.\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Core Monte Carlo Tree Search algorithm.\n",
    "    To decide on an action, we run N simulations, always starting at the root of\n",
    "    the search tree and traversing the tree according to the UCB formula until we\n",
    "    reach a leaf node.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def play_game(config: MuZeroConfig, network: Network):\n",
    "        game = config.new_game()\n",
    "\n",
    "        while not game.done and len(game.board.game_history) < config.max_moves:\n",
    "            # At the root of the search tree we use the representation function to\n",
    "            # obtain a hidden state given the current observation.\n",
    "            root = Node(0)\n",
    "            current_observation = game.get_observation()\n",
    "            MCTS.expand_node(root,\n",
    "                        game.to_play(),\n",
    "                        game.legal_actions(),\n",
    "                        network.initial_inference(current_observation))\n",
    "            MCTS.add_exploration_noise(config, root)\n",
    "\n",
    "            # We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "            # model learned by the network.\n",
    "            MCTS.run_mcts(config,root, game, network)\n",
    "            action = MCTS.select_action(config, len(game.action_history), root, network)\n",
    "            game.store_search_statistics(root)\n",
    "            #print(f\"move{action} from Player {game.board.player}\")\n",
    "            game.apply(action)\n",
    "        #print(f\"simulated game! Winner is Player {game.utils.evaluate_winner(game.board.board_grid)}\")\n",
    "        return game\n",
    "\n",
    "    # Core Monte Carlo Tree Search algorithm.\n",
    "    # To decide on an action, we run N simulations, always starting at the root of\n",
    "    # the search tree and traversing the tree according to the UCB formula until we\n",
    "    # reach a leaf node.\n",
    "    @staticmethod\n",
    "    def run_mcts(config: MuZeroConfig,\n",
    "                 root: Node,\n",
    "                 game,\n",
    "                 network: Network):\n",
    "\n",
    "        min_max_stats = MinMaxStats()\n",
    "        game_copy = copy.deepcopy(game)\n",
    "        for _ in range(config.num_simulations):\n",
    "\n",
    "            history = game_copy.get_action_history()\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            while node.expanded():\n",
    "                action, node = MCTS.select_child(config, node, min_max_stats)\n",
    "                game_copy.apply(action)\n",
    "                history.add_action(action)\n",
    "                search_path.append(node)\n",
    "\n",
    "            # Inside the search tree we use the dynamics function to obtain the next\n",
    "            # hidden state given an action and the previous hidden state.\n",
    "            parent = search_path[-2]\n",
    "            network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                         history.last_action())\n",
    "            (MCTS.expand_node(node, history.to_play(), game_copy.legal_actions(), network_output))\n",
    "            MCTS.backpropagate(search_path,\n",
    "                          network_output.value,\n",
    "                          history.to_play(),\n",
    "                          config.discount,\n",
    "                          min_max_stats)\n",
    "            if game_copy.done:\n",
    "                break\n",
    "        del game_copy\n",
    "    @staticmethod\n",
    "    def softmax_sample(distribution, temperature: float):\n",
    "        visit_counts = np.array([visit_counts for _ , visit_counts in distribution])\n",
    "        actions = np.array([actions for actions , _ in distribution])\n",
    "        visit_counts_exp = np.exp(visit_counts)\n",
    "        policy = visit_counts_exp / np.sum(visit_counts_exp)\n",
    "        policy = (policy ** (1 / temperature)) / (policy ** (1 / temperature)).sum()\n",
    "        action_index = np.random.choice(actions, p=policy)\n",
    "\n",
    "        return action_index\n",
    "    @staticmethod\n",
    "    def select_action(config,\n",
    "                      num_moves: int,\n",
    "                      node: Node,\n",
    "                      network: Network) -> Action:\n",
    "        visit_counts = [(action,child.visit_count) for action, child in node.children.items()]\n",
    "        t = config.visit_softmax_temperature_fn(num_moves=num_moves, training_steps=network.training_steps())\n",
    "        action = Action(MCTS.softmax_sample(visit_counts, t))\n",
    "        return action\n",
    "\n",
    "    # Select the child with the highest UCB score.\n",
    "    @staticmethod\n",
    "    def select_child(config: MuZeroConfig, node: Node, min_max_stats: MinMaxStats):\n",
    "        _, action, child = max(\n",
    "            (MCTS.ucb_score(config, node, child, min_max_stats), action, child) for action, child in node.children.items())\n",
    "        return action, child\n",
    "\n",
    "    # The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
    "    @staticmethod\n",
    "    def ucb_score(config: MuZeroConfig, parent: Node, child: Node, min_max_stats: MinMaxStats) -> float:\n",
    "        pb_c = math.log((parent.visit_count + config.pb_c_base + 1) / config.pb_c_base) + config.pb_c_init\n",
    "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "        prior_score = pb_c * child.prior\n",
    "        if child.visit_count > 0:\n",
    "            value_score = min_max_stats.normalize(child.reward + config.discount * child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "        return prior_score + value_score\n",
    "\n",
    "    # We expand a node using the value, reward and policy prediction obtained from the neural network.\n",
    "    @staticmethod\n",
    "    def expand_node(node: Node, to_play: Player, actions: List[Action], network_output: NetworkOutput):\n",
    "        node.to_play = to_play\n",
    "        node.hidden_state = network_output.hidden_state\n",
    "        node.reward = network_output.reward\n",
    "        policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "        policy_sum = sum(policy.values())\n",
    "        for action, p in policy.items():\n",
    "            node.children[action] = Node(p / policy_sum)\n",
    "\n",
    "    # At the end of a simulation, we propagate the evaluation all the way up the tree to the root.\n",
    "    @staticmethod\n",
    "    def backpropagate(search_path: List[Node], value: float, to_play: Player, discount: float,\n",
    "                      min_max_stats: MinMaxStats):\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value if node.to_play == to_play else -value\n",
    "            node.visit_count += 1\n",
    "            min_max_stats.update(node.value())\n",
    "\n",
    "            value = node.reward + discount * value\n",
    "\n",
    "    # At the start of each search, we add dirichlet noise to the prior of the root\n",
    "    # to encourage the search to explore new actions.\n",
    "    @staticmethod\n",
    "    def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "        actions = list(node.children.keys())\n",
    "        noise = np.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "        frac = config.root_exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "class SharedStorage(object):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.network = Network(config)\n",
    "\n",
    "    def save_network(self,network):\n",
    "        self.network = network\n",
    "\n",
    "    def latest_network(self):\n",
    "        return self.network\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, config: MuZeroConfig):\n",
    "        self.window_size = config.window_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def save_game(self, game):\n",
    "        if len(self.buffer) > self.window_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "\n",
    "    def sample_batch(self, num_unroll_steps: int, td_steps: int, action_space_size: int):\n",
    "        games = [self.sample_game() for _ in range(self.batch_size)]\n",
    "        game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "        return [(g.observation_list[i],\n",
    "                 g.action_history[i:i + num_unroll_steps],\n",
    "                 g.make_target(state_index=i,num_unroll_steps=num_unroll_steps, td_steps=td_steps, to_play=g.to_play(), action_space_size=action_space_size))\n",
    "                for (g, i) in game_pos]\n",
    "\n",
    "    def sample_game(self):\n",
    "        # Sample game from buffer either uniformly or according to some priority.\n",
    "        return self.buffer[np.random.choice(range(len(self.buffer)))]\n",
    "\n",
    "    def sample_position(self, game) -> int:\n",
    "        # Sample position from game either uniformly or according to some priority.\n",
    "        return np.random.choice(range(len(game.rewards) - 1))\n",
    "\n",
    "    def last_game(self):\n",
    "        return self.buffer[-1]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87ebe124c0633d03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"TensorFlow is using GPU: \", tf.test.is_gpu_available())\n",
    "\n",
    "# Each self-play job is independent of all others; it takes the latest network\n",
    "# snapshot, produces a game and makes it available to the training job by\n",
    "# writing it to a shared replay buffer.\n",
    "\n",
    "def run_selfplay(config: MuZeroConfig,\n",
    "                 storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer,\n",
    "                 iteration: int):\n",
    "    mcts = MCTS()\n",
    "    for _ in tqdm(range(config.batch_size), desc=\"Selfplay Batch creation\", position=1, leave=True):\n",
    "        network = storage.latest_network()\n",
    "        game = mcts.play_game(config,network)\n",
    "        replay_buffer.save_game(game)\n",
    "    print(f\"Batch {iteration} creation completed. Starting training...\")\n",
    "\n",
    "# Each game is produced by starting at the initial board position, then\n",
    "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
    "# of the game is reached.\n",
    "\n",
    "\n",
    "\n",
    "def scalar_loss(prediction, target) -> float:\n",
    "    # MSE in board games, cross entropy between categorical values in Atari.\n",
    "    return tf.losses.mean_squared_error(target, prediction)\n",
    "\n",
    "\n",
    "def scale_gradient(tensor, scale: float):\n",
    "    # Scales the gradient for the backward pass.\n",
    "    return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)\n",
    "\n",
    "\n",
    "def update_weights(optimizer: tf.keras.optimizers.Optimizer, network: Network, batch, weight_decay: float):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for (image, actions, targets) in batch:\n",
    "\n",
    "            # Initial step, from the real observation.\n",
    "            value, reward, _, policy_t, hidden_state = network.initial_inference(image)\n",
    "            predictions = [(1.0, value, reward, policy_t)]\n",
    "\n",
    "            # Recurrent steps, from action and previous hidden state.\n",
    "            for action in actions:\n",
    "                value, reward, _, policy_t, hidden_state = network.recurrent_inference(hidden_state, Action(action))\n",
    "                predictions.append((1.0 / len(actions), value, reward, policy_t))\n",
    "\n",
    "                hidden_state = scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "            for k, (prediction, target) in enumerate(zip(predictions, targets)):\n",
    "\n",
    "                gradient_scale, value, reward, policy_t = prediction\n",
    "                target_value, target_reward, target_policy = target\n",
    "\n",
    "                l_a = scalar_loss(value, [target_value])\n",
    "\n",
    "                if k > 0:\n",
    "                    l_b = tf.dtypes.cast(scalar_loss(reward, [target_reward]), tf.float32)\n",
    "                else:\n",
    "                    l_b = 0\n",
    "\n",
    "                if target_policy == []:\n",
    "                    l_c = 0\n",
    "                else:\n",
    "                    # l_c = tf.nn.softmax_cross_entropy_with_logits(logits=policy_t, labels=target_policy)\n",
    "                    cce = keras.losses.CategoricalCrossentropy()\n",
    "                    l_c = cce([target_policy], policy_t)\n",
    "\n",
    "                l = l_a + l_b + l_c\n",
    "\n",
    "                loss += scale_gradient(l, gradient_scale)\n",
    "\n",
    "        loss /= len(batch)\n",
    "\n",
    "        for weights in network.get_weights():\n",
    "            loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # optimizer.minimize(loss) # this is old Tensorflow API, we use GradientTape\n",
    "\n",
    "    gradients = tape.gradient(loss, [network.representation.trainable_variables,\n",
    "                                     network.dynamics.trainable_variables,\n",
    "                                     network.policy.trainable_variables,\n",
    "                                     network.value.trainable_variables,\n",
    "                                     network.reward.trainable_variables])\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients[0], network.representation.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[1], network.dynamics.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[2], network.policy.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[3], network.value.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients[4], network.reward.trainable_variables))\n",
    "\n",
    "    return network, loss\n",
    "\n",
    "def train_network(config: MuZeroConfig, storage: SharedStorage, replay_buffer: ReplayBuffer, iterations: int):\n",
    "    network = storage.latest_network()\n",
    "    learning_rate = config.lr_init * config.lr_decay_rate ** (iterations / config.lr_decay_steps)\n",
    "    optimizer = keras.optimizers.legacy.Adam()\n",
    "\n",
    "\n",
    "    batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps, config.action_space_size)\n",
    "    network, loss = update_weights(optimizer, network, batch, config.weight_decay)\n",
    "\n",
    "    network.tot_training_steps += 1\n",
    "\n",
    "    #save model every 25 episodes\n",
    "    if iterations % 25 == 0 and iterations >0:\n",
    "        network.backup_count += 1\n",
    "        network.save_network_deepcopy(model_name=config.model_name)\n",
    "\n",
    "    storage.save_network(network)\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def launch_job(f, *args):\n",
    "    f(*args)\n",
    "\n",
    "\n",
    "def muzero(config: MuZeroConfig):\n",
    "\n",
    "    model_name = config.model_name\n",
    "    storage = SharedStorage(config)\n",
    "    replay_buffer = ReplayBuffer(config)\n",
    "    losses = []\n",
    "    print(f\"Starting Selfplay for {config.model_name}! Batch size is {config.batch_size}.\")\n",
    "\n",
    "    for i in tqdm(range(config.training_episodes), desc=f\"Training episodes for {config.model_name}\", position=0):\n",
    "\n",
    "        # self-play\n",
    "        launch_job(run_selfplay, config, storage, replay_buffer, i)\n",
    "\n",
    "        # training\n",
    "        loss = train_network(config, storage, replay_buffer, i)\n",
    "\n",
    "        # print and plot loss\n",
    "        print('Loss: ' + str(loss))\n",
    "        losses.append(loss[0])\n",
    "        plt.plot(losses, label=f\"Loss {model_name}\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"batches processed\")\n",
    "        plt.show()\n",
    "        plt.savefig(\"loss_plot_\" + model_name + \".png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    muzero(make_Go7x7_config())\n",
    "    muzero(make_Go9x9_config())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b40b9dae0cf17a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c29955590137e70e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "563d00fe50671d47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
